Adding a worker node to the Kubernetes cluster
You can add workers to an existing Kubernetes cluster.

Before you begin
Before you add a worker node, you must ensure that the version of the Kubernetes RPM files in the $HOME/fci-install-kit/helm/distrib/ directory match the version of Kubernetes installed. To do so, follow these steps:

On the Kubernetes master node, find the version of Kubernetes that is installed:
kubectl version --short

Results are similar to the following:
Client Version: v1.14.1
Server Version: v1.14.1

Check that your version matches the versions of the Kubernetes RPM files in the $HOME/fci-install-kit/helm/distrib/ directory. For example, the kubectl and kubeadm RPM files appear similar to the following:
`5c6cb3beb5142fa21020e2116824ba77a2d1389a3321601ea53af5ceefe70ad1-kubectl-1.14.1-0.x86_64.rpm`
`9e1af74c18311f2f6f8460dbd1aa3e02911105bfd455416381e995d8172a0a01-kubeadm-1.14.1-0.x86_64.rpm`

Attention: If the version returned by the kubectl command is equal to or higher than the packaged version, continue to follow steps in this procedure. If the version returned is lower than what is packaged, then you must first upgrade the installed Kubernetes by installing APAR PH09019 located on IBM Fix Central (Download | Readme).
Procedure
To add a worker node to the Kubernetes cluster:

Edit the $HOME/fci-install-kit/helm/install.hosts.properties file and add the new worker node. For example, if the next incremental worker node is worker.3, add a new entry similar to the following:
master.ip=172.18.160.90
master.fqdn=fci-master.ibm.com
master.root_password=passw0rd
master.external_ip=9.30.15.9

worker.1.ip=9.0.10.1 
worker.1.fqdn=fci-wkr1.ibm.com
worker.1.root_password=passw0rd

worker.2.ip=9.0.10.2
worker.2.fqdn=fci-wkr2.ibm.com
worker.2.root_password=passw0rd

worker.3.ip=9.0.10.3
worker.3.fqdn=fci-wkr3.ibm.com
worker.3.root_password=passw0rd

Run the following command:
./install.sh --add-workers

This command finds and initializes any new Kubernetes worker nodes and joins them to the cluster.
Notes:
The installer does not apply any taints or labels to the new nodes.
Pods do not automatically move to the new nodes until something causes a pod to end.
To verify that the worker node was added to the cluster:
kubectl get nodes

Results appear similar to the following. Verify that the new node is listed.
NAME                   STATUS   ROLES    AGE     VERSION
fci-master.ibm.com     Ready    master   3h44m   v1.14.1
fci-wkr1.ibm.com       Ready    <none>   81s     v1.14.1
fci-wkr2.ibm.com       Ready    <none>   81s     v1.14.1
fci-wkr3.ibm.com       Ready    <none>   81s     v1.14.1

################################
################################

Debugging worker nodes with Kubernetes API
Last updated: 2021-08-13Contribute in GitHub:Open doc issue|Edit topic

If you have access to the cluster, you can debug the worker nodes by using the Kubernetes API on the Node resource.

Before you begin, make sure that you have the Manager service access role in all namespaces for the cluster, which corresponds to the cluster-admin RBAC role.

Log in to your account. If applicable, target the appropriate resource group. Set the context for your cluster.

List the worker nodes in your cluster and note the NAME of the worker nodes that are not in a Ready STATUS. Note that the NAME is the private IP address of the worker node.


kubectl get nodes
Describe the each worker node, and review the Conditions section in the output.

Type: The type of condition that might affect the worker node, such as memory or disk pressure.
LastTransitionTime: The most recent time that the status was updated. Use this time to identify when the issue with your worker node began, which can help you further troubleshoot the issue.

kubectl describe node <name>
Check the usage of the worker nodes.

In the Allocated resources output of the previous command, review the workloads that use the worker node's CPU and memory resources. You might notice that some pods do not set resource limits, and are consuming more resources than you expected. If so, adjust the resource usage of the pods.
Review the percentage of usage of CPU and memory across the worker nodes in your cluster. If the usage is consistently over 80%, add more worker nodes to the cluster to support the workloads.
Check for custom admission controllers that are installed in your cluster. Admission controllers often block required pods from running, which might make your worker nodes enter a critical state. If you have custom admission controllers, try removing them with kubectl delete. Then, check if the worker node issue resolves.


kubectl get mutatingwebhookconfigurations --all-namespaces

kubectl get validatingwebhookconfigurations --all-namespaces
If you configured log forwarding, review the node-related logs from the following paths.

/var/log/containerd.log
/var/log/kubelet.log
/var/log/kube-proxy.log
/var/log/syslog
Check that a workload deployment does not cause the worker node issue.

Taint the worker node with the issue.

kubectl taint node NODEIP ibm-cloud-debug-isolate-customer-workload=true:NoExecute
Make sure that you deleted any custom admission controllers as described in step 5.
Restart the worker node.
Classic: Reload the worker node.

ibmcloud ks worker reload -c <cluster_name_or_ID> --worker <worker_ID>
VPC: Replace the worker node.

ibmcloud ks worker replace -c <cluster_name_or_ID> --worker <worker_ID> --update
Wait for the worker node to finish restarting. If the worker node enters a healthy state, the issue is likely caused by a workload.
Schedule one workload at a time onto the worker node to see which workload causes the issue. To schedule the workloads, add the following toleration.
tolerations:
- effect: NoExecute
  key: ibm-cloud-debug-isolate-customer-workload
  operator: Exists
```    6. After you identify the workload that causes the issue, continue with [Debugging app deployments](/docs/containers?topic=containers-debug_apps).

################################
################################
